{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8d1Oz/YqJXhM13UVbPQ0U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task: Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."],"metadata":{"id":"R_Eu1bmP3X3m"}},{"cell_type":"code","source":["from sklearn import datasets\n","iris = datasets.load_iris()\n","list(iris.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXjOrJamWfW_","executionInfo":{"status":"ok","timestamp":1715694453216,"user_tz":-120,"elapsed":2089,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}},"outputId":"79abf847-1c76-4d37-d237-6adff7fc5819"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data',\n"," 'target',\n"," 'frame',\n"," 'target_names',\n"," 'DESCR',\n"," 'feature_names',\n"," 'filename',\n"," 'data_module']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","X = iris['data'][:, (2, 3)]   # petal length, petal width\n","X = np.c_[np.ones([len(X), 1]), X]  # add bias term\n","y = iris['target']"],"metadata":{"id":"WMa4Ml8bWsud","executionInfo":{"status":"ok","timestamp":1715696105378,"user_tz":-120,"elapsed":240,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["np.random.seed(2042)\n","\n","test_ratio = 0.2\n","validation_ratio = 0.2\n","total_size = len(y)\n","\n","test_size = int(total_size * test_ratio)\n","validation_size = int(total_size * validation_ratio)\n","train_size = total_size - test_size - validation_size\n","\n","rnd_indices = np.random.permutation(total_size)\n","\n","X_train = X[rnd_indices[:train_size]]\n","X_valid = X[rnd_indices[train_size:-test_size]]\n","X_test = X[rnd_indices[-test_size:]]\n","y_train = y[rnd_indices[:train_size]]\n","y_valid = y[rnd_indices[train_size:-test_size]]\n","y_test = y[rnd_indices[-test_size:]]\n"],"metadata":{"id":"t80tNZ43XLBq","executionInfo":{"status":"ok","timestamp":1715696161238,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def one_hot_encoding(y):\n","    n_classes = y.max() + 1\n","    m = len(y)\n","    Y_one_hot = np.zeros((m, n_classes))\n","    Y_one_hot[np.arange(m), y] = 1\n","    return Y_one_hot\n","\n","\n","def train(X_train, y_train, X_valid, y_valid):\n","    y_train_encoded = one_hot_encoding(y_train)\n","    y_valid_encoded = one_hot_encoding(y_valid)\n","    theta = gradient_descent(X_train, y_train_encoded, X_valid, y_valid_encoded)\n","    print()\n","    print(theta, end='\\n\\n')\n","    return theta\n","\n","\n","def gradient_descent(X_train, y_train, X_valid, y_valid, eta=0.1,\n","                     n_iterations=5000, epsilon=1e-7, alpha=0.1):\n","    m = X_train.shape[0]  # number of rows\n","    output_number = len(np.unique(y_train, axis=0))\n","    input_number = X_train.shape[1]\n","    theta = np.random.randn(input_number, output_number)\n","    best_loss = np.infty\n","\n","    for iteration in range(1, n_iterations+1):\n","        z = X_train.dot(theta)\n","        y_train_prob = softmax(z)\n","        error = y_train_prob - y_train\n","        gradients = 1/m * X_train.T.dot(error)\n","        theta -= eta * gradients\n","\n","        valid_z = X_valid.dot(theta)\n","        y_valid_prob = softmax(valid_z)\n","        # logarithm will alway_trains be negative, so we add -\n","        cross_entropy_train_loss = -np.mean(\n","            np.sum(y_valid * np.log(y_valid_prob + epsilon), axis=1)\n","            )\n","        l2_loss = 1/2 * np.sum(np.square(theta[1:]))\n","        loss = cross_entropy_train_loss + alpha * l2_loss\n","\n","        if iteration % 500 == 0:\n","            print(iteration, loss)\n","        if loss < best_loss:\n","            best_loss = loss\n","        else:\n","            print(iteration-1, best_loss)\n","            print(iteration, loss, \"early stopping!\")\n","            break\n","\n","    return theta\n","\n","\n","def softmax(z):\n","    y_prob = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n","    return y_prob\n","\n","\n","theta = train(X_train, y_train, X_valid, y_valid)\n"],"metadata":{"id":"xHSIqv2c3e_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715699559135,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}},"outputId":"5443ec06-d5f8-4671-8b7d-23aeb94ecbc3"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["220 1.2509798944267723\n","221 1.2509816824306497 early stopping!\n","\n","[[ 1.37621026 -0.3677057  -2.15691583]\n"," [ 0.4160048   0.08952304  0.36678774]\n"," [-1.32032754  2.27451352  2.68778647]]\n","\n"]}]},{"cell_type":"code","source":["logits = X_valid.dot(theta)\n","Y_proba = softmax(logits)\n","y_predict = np.argmax(Y_proba, axis=1)\n","\n","accuracy_score = np.mean(y_predict == y_valid)\n","accuracy_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jusxE5Wm3nh","executionInfo":{"status":"ok","timestamp":1715699569205,"user_tz":-120,"elapsed":242,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}},"outputId":"0638edf4-4a63-41dc-ca9d-cd76c8ce521b"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["logits = X_test.dot(theta)\n","Y_proba = softmax(logits)\n","y_predict = np.argmax(Y_proba, axis=1)\n","\n","accuracy_score = np.mean(y_predict == y_test)\n","accuracy_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_V8mlcrm8Wu","executionInfo":{"status":"ok","timestamp":1715699575168,"user_tz":-120,"elapsed":325,"user":{"displayName":"Tomas David","userId":"17081504748808486304"}},"outputId":"f81f7d90-9643-49dc-9d22-3117ee02138b"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9"]},"metadata":{},"execution_count":43}]}]}